\chapter{Introduction}

The use of artificial intelligence (AI) in extreme edge devices such as wireless sensor nodes (WSNs) will greatly benefit the scalability and application space of such nodes. AI can be applied to solve problems with clustering, data routing, and most importantly it can be used to reduce the volume of data transmission via data compression or making conclusions from data within the node itself \cite{alsheikh2014machine}.

However, since devices in the extreme edge are constrained to work with extremely low amounts of memory and energy \cite{Ma_2019}, even the simplest AI models are difficult to execute with typical sequential processors. WSNs have memories in the order of kB and clock speeds in the order of kHz to MHz due to energy constraints, rendering them unable to run state-of-the art AI applications.

A promising hardware approach allowing the use of AI in low-power edge devices is in-memory computing (IMC) \cite{Patterson_1997}. IMC allows very high energy savings compared to other approaches by bypassing the most energy-expensive and time-consuming part of AI processing: memory accesses. Analog IMC (AIMC) with memristors has proven to be fast and efficient at multiply-and-accumulate operations (MACs) which are by far the most common operation used by AI software. Additionally, since memristors are nonvolatile memory devices, they are particularly robust to energy interruptions from ultra-low power situations in WSN. 

Several problems bar the use Analog IMC in extreme edge devices. The memristive devices typically used by Analog IMC architectures typically report much higher write energy costs than read energy costs \cite{}. This poses the need to amortize the write energy cost over as many MAC operations as possible in the AIMC architecture.

Typical Digital DNN accelerators also seek to amortize the cost of writes in memory by implementing as much data reuse as possible. This is made possible by spatial arrangement of parallel processing elements (PEs) in the architecture. The spatial arrangement of PEs allows for data reuse by allowing the same data to be used by multiple PEs at the same time. DNN compilers figure out the arrangement of data and computational workflow for DNNs in digital accelerators that best take advantage of the spatial arrangement of PEs to maximize data reuse. 

% Many DNN compiler works investigate significant data reuse 