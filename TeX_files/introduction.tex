\chapter{Introduction}

The use of deep neural networks (DNNs) in extreme edge devices such as wireless sensor nodes (WSNs) will greatly benefit the scalability and application space of such nodes. AI can be applied to solve problems with clustering, data routing, and most importantly it can be used to reduce the volume of data transmission via data compression or making conclusions from data within the node itself \cite{alsheikh2014machine}.

However, since devices in the extreme edge are constrained to work with extremely low amounts of memory and energy, even the simplest AI models are difficult to execute with typical sequential processors. WSNs have memories in the order of kB and clock speeds in the order of kHz to MHz due to energy constraints, rendering them unable to run state-of-the art AI applications.

Chapter \ref{chapter:background} discusses the leading solutions to this problem. As of the recent decade, two main solutions to enabling AI on the edge have emerged: the software-side development of more efficient AI algorithms and the hardware-side design and fabrication of more efficient application-specific integrated circuits (ASICs). 

A field called TinyML leads the software-side push. TinyML strives to develop ML models and software frameworks that are more efficient in terms of memory and energy usage, such as quantization \cite{jacob2018quantization}, neural architecture search (NAS) \cite{lin2020mcunet}, and depthwise convolutions \cite{sandler2018mobilenetv2}. 

Hardware accelerators instead are the main results of the hardware-side push. Hardware accelerators are machines  that can run ML models at extremely high energy efficiencies. One of the most promising types of hardware accelerators for edge AI are in-memory computing (IMC) accelerators \cite{sebastian2020memory}. IMC accelerators allow very high energy efficiencies compared to other approaches by eliminating a significant amount of the memory accesses required in model inference.  IMC does this by performing computations using the memory effectively merging both the memory read and computation. Analog IMC (AIMC) has proven to be the fastest and most energy-efficient way to accelerate multiply-and-accumulate operations (MACs) which are by far the most common operation used by AI software.

Chapter \ref{chapter:mapping} discusses DNN mappers for AIMC and the current limitations of the existing mappers. DNN mappers are responsible for taking a DNN model and mapping it to the hardware. By analyzing the computational graph of the DNN, the DNN mappers can essentially create an equivalent computational graph that the hardware accelerator can run. A rudimentary mapping of a DNN could be to map each layer of the DNN as a single matrices, then, running each matrix sequentially in the AIMC accelerator. We importantly note that it is possible to map multiple matrices at the same time in AIMC accelerators, but existing DNN mappers do not do this.

Existing DNN mappers seek to optimize the latency and energy efficiency of running DNN models on hardware accelerators. To this end, they tend to limit themselves to single-mapping schemes to simplify the complexity of the optimization problem \cite{mei2021zigzag,symons2024stream,andrulis2024cimloop,lammie2024lionheart,chen2018neurosim}. This means that they only map a single layer of the DNN into the AIMC memory array at a time. However, this results in low utilization of the AIMC memory array as it does not take advantage of the fact that AIMC accelerators can hold multiple matrices at a time. 

Mapping multiple matrices at the same time reduces the number of writes needed to be done to the AIMC memory array. This is a significant advantage as AIMC accelerators typically use memory devices that have a significantly high write energies. Hence, AIMC accelerators would benefit greatly from a DNN mapper that can optimize the mapping of multiple layers in AIMC accelerators with interlayer data reuse.

Multi-matrix mappings \cite{wanneurram,garofalo2022heterogeneous,zhou2022ml} have been demonstrated for single models before. However, these multi-matrix mappings are not results of a DNN mapper but rather are ad-hoc, handcrafted demonstrations of the feasibility of the AIMC accelerators being proposed. Being ad-hoc, these works provide no analysis of mapping optimization, energy efficiency gains, and latency. Not only that, but they only demonstrate the final mapping of a single model.

Chapter \ref{chapter:marp} introduces MARP, a DNN compiler codesigned with QRAcc that optimizes DNN mappings in AIMC architectures by solving rectangular bin packing algorithms to pack DNN layers as tightly as possible into the AIMC accelerator. We show that MARP can achieve optimal memory utilization for a given AIMC accelerator for the modern DNN workloads in MLPerfTiny \cite{}. MARP is the first DNN compiler to optimize for AIMC architectures with multi-matrix mappings on a single AIMC core. We show that MARP achieves an average $xxx$ reduction in write energy consumption and $xxx$ reduction in overall inference latency compared to a naive single-mapping scheme for the MLPerfTiny models for edge inference \cite{banbury2021mlperf}.

Chapter \ref{chap:qracc} discusses QRAcc, a hybrid DNN accelerator architecture with a weight-stationary digital accelerator (called WSAcc) and charge-redistribution SRAM AIMC bank (called SeqAcc) that can support the workloads of modern DNNs designed for the edge. We designed QRAcc in order to verify the results of MARP and fulfill specific design requirements of an accelerator for MARP. QRAcc achieves energy efficiency comparable to SOTA at 509 TOPS/W when using its AIMC core and 81.36 TOPS/W when using its digital core.