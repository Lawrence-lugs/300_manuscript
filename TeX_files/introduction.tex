\chapter{Introduction}

The use of deep neural networks (DNNs) in extreme edge devices such as wireless sensor nodes (WSNs) will greatly benefit the scalability and application space of such nodes. AI can be applied to solve problems with clustering, data routing, and most importantly it can be used to reduce the volume of data transmission via data compression or making conclusions from data within the node itself \cite{alsheikh2014machine}.

However, since devices in the extreme edge are constrained to work with extremely low amounts of memory and energy, even the simplest AI models are difficult to execute with typical sequential processors. WSNs have memories in the order of kB and clock speeds in the order of kHz to MHz due to energy constraints, rendering them unable to run state-of-the art AI applications.

As of the recent decade, two main solutions to the growing compute needs of AI have emerged: the development of more efficient AI algorithms and the design and fabrication of more efficient application-specific integrated circuits (ASICs) \cite{}. The former has led to the emergence of new AI algorithms that are more efficient in terms of memory and energy usage, such as neural network quantization \cite{jacob2018quantization}, neural architecture search (NAS) \cite{lin2020mcunet}, and depthwise convolutions \cite{sandler2018mobilenetv2}. The latter has led to the emergence of hardware accelerators that take advantage of data usage patterns in DNN computational graphs to reduce memory accesses. Memory accesses are the most energy-expensive and time-consuming part of AI processing \cite{horowitz20141}. 

One of the most promising hardware solutions to edge AI is in-memory computing (IMC) \cite{sebastian2020memory}. IMC allows very high energy savings compared to other approaches by eliminating half of the memory accesses required by performing computations using the memory array. Analog IMC (AIMC) with memristors has proven to be fast and efficient at multiply-and-accumulate operations (MACs) which are by far the most common operation used by AI software. Additionally, since memristors are nonvolatile memory devices, they are particularly robust to energy interruptions from ultra-low power situations in WSN. 

DNN compilers are responsible for taking a DNN model and mapping it to the hardware \cite{li2020deep}. By analyzing the computational graph of the DNN, the DNN compiler can essentially create an equivalent computational graph- a "mapping"- that is compatible with and optimal for the hardware to map the DNN to the hardware accelerator. ASIC DNN accelerators emerged faster than the accompanying DNN compilers in the past decade (2015-2025). While the gap has mostly closed for compiling for digital accelerators \cite{mei2021zigzag}, the same cannot be said for analog accelerators \cite{chen2018neurosim}. The majority of papers on analog accelerators are still tested with handcrafted mappings \cite{wanneurram,garofalo2022heterogeneous,zhou2022ml} or suboptimal single-layer mappings. We find that there are three main challenges for DNN compilation and mapping for AIMC accelerators.

% cite cimloop and neurosim above

Firstly, an AIMC DNN compiler must amortize the write energy cost over as many MAC operations as possible in the AIMC architecture, moreso than in regular digital accelerators. Analog IMC banks typically report much higher write energy costs than read energy costs \cite{sebastian2020memory} than digital accelerators. DNN compilers for digital accelerators typically treat feature maps and weights almost equally, prioritizing whichever causes the highest amount of data reuse \cite{mei2023design}. In contrast, there exists a large discrepancy in write energy to the AIMC memory array (typically used for weights) vs the cost of moving data to and from peripheral buffers (typically used for activations) due to the use of special memory cells and other peripheral elements like the ADC and adder trees. Not only that, but AIMC also incentivizes doubling down on weight reuse due to its essentially zero read energy cost for weights once they are in the AIMC memory array.

Secondly, not all the multiply-accumulate layers of modern DNNs benefit from AIMC. AIMC arrays only support general matrix multiplies (GEMMs). Generally, the solution is typically to transform other operations into matrix operations which work for most DNN layers. However, modern DNNs also employ layers that use special types of convolutions like depthwise convolutions \cite{sandler2018mobilenetv2} that end up transforming into to impractically large sparse matrices or thousands of extremely small (~1x9) matrices. Both of these cases are inefficient for AIMC architectures. The former case leads to a large number of writes and the latter case leads to an impossibly large latency due to the matrices' inability to share their inputs. Certain dataflow architectures for digital accelerators like the row-stationary dataflow support depthwise convolutions natively. Due to that, heterogenous accelerators that contain both digital and AIMC processing elements have been designed \cite{garofalo2022heterogeneous,houshmand2022diana}. 

Thirdly, while efficient mapping of single DNN layers is mostly solved \cite{mei2021zigzag} and interlayer mapping for digital accelerators has made significant progress \cite{mei2023defines}, optimizing mappings of multiple DNN layers on AIMC architectures is still an open problem. In DNN compilation, some layers map into matrices that are larger than the AIMC memory array while others are much smaller than the memory array size. All existing DNN compiler works with interlayer scope avoid mapping the more than a single layer into an AIMC memory array or digital processing element (PE) array at the same time to simplify the work. In the rest of this work, we will refer to this as "one-to-one restriction". 

% maybe there should be a long paragraph about QRAcc and then another one about DARP

In this work, we introduce QRAcc, a heterogenous DNN accelerator architecture with a row-stationary digital accelerator and charge-redistribution SRAM AIMC bank that can support the workloads of modern DNNs designed for the edge at high utilization. We also introduce MARP, a DNN compiler codesigned with QRAcc that optimizes DNN mappings in heterogenous AIMC architectures by solving rectangular bin packing algorithms to pack DNN layers as tightly as possible into the AIMC accelerator. We show that MARP can achieve optimal memory utilization for a given heterogenous accelerator architecture for the modern DNN workloads in MLPerfTiny \cite{}. MARP is the first DNN compiler to optimize for AIMC architectures with interlayer mappings without being restricted to single-layer mappings. We verify the results of MARP with implementations and measurements of energy efficiency in QRAcc implemented in 22nm FDSOI technology. We find that different settings for MARP shows design tradeoffs that optimize for different aspects such as utilization, latency, and energy efficiency. Lastly, we provide optimal QRAcc configurations for the MLPerfTiny workloads. 
