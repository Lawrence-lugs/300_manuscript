\chapter*{Abstract}
    
Analog in-memory computing (AIMC) accelerators offer exceptional energy and area efficiency for deep neural network (DNN) inference at the extreme edge, but their practical deployment is limited by low array utilization and high energy costs associated with weight writes on the typically energy-expensive nonvolatile memory cells. Existing DNN mappers all limit themselves to mapping a single matrix in the AIMC memory array at any given time for simplicity. 

In this work, we introduce MARP, a novel DNN mapping framework that leverages rectangular bin packing algorithms to maximize AIMC array utilization by packing multiple layers—across and within models—into a single memory array with interlayer weight reuse. We further present QRAcc, a fully integrated hybrid accelerator supporting both SRAM-based AIMC and a weight-stationary digital accelerator, designed to exploit MARP’s mapping strategies. 

QRAcc achieves state-of-the-art energy efficiencies of 509.3 TOPS/W at 6.55 TOPS for AIMC and 81.356 TOPS/W at 5.76 GOPS for digital workloads. We demonstrate using the MLPerfTiny models that MARP increases AIMC memory utilization by up to +64.7\%, reduces memory writes by up to 81.25\%, and achieves up to 1723.2\% higher energy effiency, 59.52\% lower energy consumption, and 57.87\% lower inference latency on QRAcc. 

The dense packings of QRAcc enable the mapping of larger models like MobileNetV2 on AIMC accelerators with many cores, such as NeuRRAM, which can fully map MobileNetV2 at 34 bins. The write optimizations of MARP also reduce the number of AIMC memory writes, which is crucial especially for novel AIMC memory types that have high write energy costs, such as RRAM and PCM. 
MARP enables system-level energy efficiencies exceeding 5 TOPS/W, validating the effectiveness of MARP and QRAcc for efficient edge AI applications. 