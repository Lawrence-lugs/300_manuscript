\chapter*{Abstract}
    
Analog in-memory computing (AIMC) accelerators offer exceptional energy and area efficiency for deep neural network (DNN) inference at the extreme edge, but their practical deployment is limited by low array utilization and high energy costs associated with weight writes on the typically energy-expensive nonvolatile memory cells. Existing DNN mappers all limit themselves to mapping a single matrix in the AIMC memory array at any given time for simplicity. In this work, we introduce MARP, a novel DNN mapping framework that leverages rectangular bin packing algorithms to maximize AIMC array utilization by packing multiple layers—across and within models—into a single memory array with interlayer weight reuse. We further present QRAcc, a fully integrated hybrid accelerator supporting both SRAM-based AIMC and a weight-stationary digital accelerator, designed to exploit MARP’s mapping strategies. Our results on MLPerfTiny models demonstrate that MARP increases AIMC memory utilization by up to +64.7\%, reduces memory writes by up to 81.25\%, and achieves up to 1723.2\% higher energy effiency, 59.52\% lower energy consumption, and 57.87\% lower inference latency on QRAcc. These advances enable system-level energy efficiencies exceeding 5 TOPS/W, validating the effectiveness of MARP and QRAcc for efficient edge AI applications. Additionally, the dense packings of QRAcc enable the mapping of larger models like MobileNetV2 on AIMC accelerators with many cores, such as NeuRRAM, which can fully map MobileNetV2 at 34 bins.