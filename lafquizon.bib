@misc{allinger2017magnetic,
  title={Magnetic shielding of perpendicular STT-MRAM},
  author={Allinger, Robert and Hofmann, Karl and Knobloch, Klaus and Strenz, Robert},
  year={2017},
  month=feb # "~7",
  publisher={Google Patents},
  note={US Patent 9,564,403}
}

@article{alsheikh2014machine,
  title={Machine learning in wireless sensor networks: Algorithms, strategies, and applications},
  author={Alsheikh, Mohammad Abu and Lin, Shaowei and Niyato, Dusit and Tan, Hwee-Pink},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={16},
  number={4},
  pages={1996--2018},
  year={2014},
  publisher={IEEE}
}

@inproceedings{antonio2017implementation,
  title={Implementation of dynamic voltage frequency scaling on a processor for wireless sensing applications},
  author={Antonio, Ryan Albert and de la Costa, Rafael Mari and Ison, Aldrin Rolf and Lim, Wesley Kaiser and Pajado, Robert Adrian and Roque, Deanne Bianca and Yutuc, Ruelle and Densing, Chris Vincent and de Leon, Maria Theresa and Rosales, Marc and others},
  booktitle={TENCON 2017-2017 IEEE Region 10 Conference},
  pages={2955--2960},
  year={2017},
  organization={IEEE}
}

@misc{apple2019designing,
  title={Designing for Privacy},
  author={Apple},
  howpublished={\url{https://developer.apple.com/videos/play/wwdc2019/708/}},
  year={2019}
}

@article{banbury2021mlperf,
  title={Mlperf tiny benchmark},
  author={Banbury, Colby and Reddi, Vijay Janapa and Torelli, Peter and Holleman, Jeremy and Jeffries, Nat and Kiraly, Csaba and Montino, Pietro and Kanter, David and Ahmed, Sebastian and Pau, Danilo and others},
  journal={arXiv preprint arXiv:2106.07597},
  year={2021}
}

@inproceedings{basu2022spiking,
  title={Spiking neural network integrated circuits: A review of trends and future directions},
  author={Basu, Arindam and Deng, Lei and Frenkel, Charlotte and Zhang, Xueyong},
  booktitle={2022 IEEE Custom Integrated Circuits Conference (CICC)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@misc{bertschinger2000introduction,
  title={Introduction to tensor calculus for general relativity},
  author={Bertschinger, Edmund},
  year={2000}
}

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusm{\~a}o, Pedro Porto Buarque and others},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}

@inproceedings{beyer2020fefet,
  title={FeFET: A versatile CMOS compatible device with game-changing potential},
  author={Beyer, Sven and D{\"u}nkel, Stefan and Trentzsch, Martin and M{\"u}ller, Johannes and Hellmich, Andreas and Utess, Dirk and Paul, Jan and Kleimaier, Dominik and Pellerin, John and M{\"u}ller, Stefan and others},
  booktitle={2020 IEEE International Memory Workshop (IMW)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}

@inproceedings{biswas2022area,
  title={An area-efficient 6T-SRAM based compute-in-memory architecture with reconfigurable SAR ADCs for energy-efficient deep neural networks in edge ML applications},
  author={Biswas, Avishek and Sanghvi, Hetul and Mehendale, Mahesh and Preet, G},
  booktitle={2022 IEEE Custom Integrated Circuits Conference (CICC)},
  pages={1--2},
  year={2022},
  organization={IEEE}
}

@inproceedings{buchel2024aihwkit,
  title={AIHWKIT-lightning: a scalable HW-aware training toolkit for analog in-memory computing},
  author={B{\"u}chel, Julian and Simon, William Andrew and Lammie, Corey and Acampa, Giovanni and El Maghraoui, Kaoutar and Le Gallo, Manuel and Sebastian, Abu},
  booktitle={NeurIPS 2024 Workshop Machine Learning with new Compute Paradigms},
  year={2024}
}

@inproceedings{cai2020rethinking,
  title={Rethinking differentiable search for mixed-precision neural networks},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2349--2358},
  year={2020}
}

@article{camus2019review,
  title={Review and benchmarking of precision-scalable multiply-accumulate unit architectures for embedded neural-network processing},
  author={Camus, Vincent and Mei, Linyan and Enz, Christian and Verhelst, Marian},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={4},
  pages={697--711},
  year={2019},
  publisher={IEEE}
}

@article{chen2016eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
  journal={IEEE journal of solid-state circuits},
  volume={52},
  number={1},
  pages={127--138},
  year={2016},
  publisher={IEEE}
}

@article{chen2018neurosim,
  title={NeuroSim: A circuit-level macro model for benchmarking neuro-inspired architectures in online learning},
  author={Chen, Pai-Yu and Peng, Xiaochen and Yu, Shimeng},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={37},
  number={12},
  pages={3067--3080},
  year={2018},
  publisher={IEEE}
}

@article{chen2019eyeriss,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@inproceedings{chih202116,
  title={16.4 An 89TOPS/W and 16.3 TOPS/mm 2 all-digital SRAM-based full-precision compute-in memory macro in 22nm for machine-learning edge applications},
  author={Chih, Yu-Der and Lee, Po-Hao and Fujiwara, Hidehiro and Shih, Yi-Chun and Lee, Chia-Fu and Naous, Rawan and Chen, Yu-Lin and Lo, Chieh-Pu and Lu, Cheng-Han and Mori, Haruki and others},
  booktitle={2021 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={64},
  pages={252--254},
  year={2021},
  organization={IEEE}
}

@article{chiu20204,
  title={A 4-Kb 1-to-8-bit configurable 6T SRAM-based computation-in-memory unit-macro for CNN-based AI edge processors},
  author={Chiu, Yen-Cheng and Zhang, Zhixiao and Chen, Jia-Jing and Si, Xin and Liu, Ruhui and Tu, Yung-Ning and Su, Jian-Wei and Huang, Wei-Hsing and Wang, Jing-Hong and Wei, Wei-Chen and others},
  journal={IEEE Journal of Solid-State Circuits},
  volume={55},
  number={10},
  pages={2790--2801},
  year={2020},
  publisher={IEEE}
}

@article{chowdhery2019visual,
  title={Visual wake words dataset},
  author={Chowdhery, Aakanksha and Warden, Pete and Shlens, Jonathon and Howard, Andrew and Rhodes, Rocky},
  journal={arXiv preprint arXiv:1906.05721},
  year={2019}
}

@inproceedings{chua2015delay,
  title={Delay variation compensation through error correction using razor},
  author={Chua, Adelson N and Maestro, Rico Jossel M and Alba, Mark Earvin V and Lofamia, Wes Vernon V and Pelayo, Bernard Raymond D and Fabay, Ken Bryan F and Jardin, John Cris F and Jocson, Kervin John C and Madamba, Joy Alinda R and Hizon, John Richard E and others},
  booktitle={2015 International Workshop on CMOS Variability (VARI)},
  pages={5--8},
  year={2015},
  organization={IEEE}
}

@article{conti2024open,
  title={Open-Source Heterogeneous SoCs for AI: The PULP Platform Experience},
  author={Conti, Francesco and Garofalo, Angelo and Rossi, Davide and Tagliavini, Giuseppe and Benini, Luca},
  journal={arXiv preprint arXiv:2412.20391},
  year={2024}
}

@article{d2024denram,
  title={DenRAM: neuromorphic dendritic architecture with RRAM for efficient temporal processing with delays},
  author={D’agostino, Simone and Moro, Filippo and Torchet, Tristan and Demira{\u{g}}, Yi{\u{g}}it and Grenouillet, Laurent and Castellani, Niccol{\`o} and Indiveri, Giacomo and Vianello, Elisa and Payvand, Melika},
  journal={Nature communications},
  volume={15},
  number={1},
  pages={3446},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{dazzi2021efficient,
  title={Efficient pipelined execution of CNNs based on in-memory computing and graph homomorphism verification},
  author={Dazzi, Martino and Sebastian, Abu and Parnell, Thomas and Francese, Pier Andrea and Benini, Luca and Eleftheriou, Evangelos},
  journal={IEEE Transactions on Computers},
  volume={70},
  number={6},
  pages={922--935},
  year={2021},
  publisher={IEEE}
}

@inproceedings{dimayuga2017study,
  title={A study on the effects of dynamic voltage and frequency scaling on an error detection block for a LoRa communications system},
  author={Dimayuga, Jahn Carroll and Fernandez, Ian Christian and Lopez, Alfonso Elias and Pangilinan, Rafael and Alarcon, Louis and de Leon, Maria Theresa and Maestro, Rico Jossel and Rosales, Marc and Densing, Chris Vincent},
  booktitle={TENCON 2017-2017 IEEE Region 10 Conference},
  pages={1538--1543},
  year={2017},
  organization={IEEE}
}

@article{dutta2021tinyml,
  title={Tinyml meets iot: A comprehensive survey},
  author={Dutta, Lachit and Bharali, Swapna},
  journal={Internet of Things},
  volume={16},
  pages={100461},
  year={2021},
  publisher={Elsevier}
}

@article{fang2023deep,
  title={A deep reinforcement learning algorithm for the rectangular strip packing problem},
  author={Fang, Jie and Rao, Yunqing and Shi, Mingliang},
  journal={PLoS One},
  volume={18},
  number={3},
  pages={e0282598},
  year={2023},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{ferro2024precision,
  title={A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing},
  author={Ferro, Elena and Vasilopoulos, Athanasios and Lammie, Corey and Le Gallo, Manuel and Benini, Luca and Boybat, Irem and Sebastian, Abu},
  booktitle={2024 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2024},
  organization={IEEE}
}

@inproceedings{fujiwara20225,
  title={A 5-nm 254-TOPS/W 221-TOPS/mm 2 fully-digital computing-in-memory macro supporting wide-range dynamic-voltage-frequency scaling and simultaneous MAC and write operations},
  author={Fujiwara, Hidehiro and Mori, Haruki and Zhao, Wei-Chang and Chuang, Mei-Chen and Naous, Rawan and Chuang, Chao-Kai and Hashizume, Takeshi and Sun, Dar and Lee, Chia-Fu and Akarvardar, Kerem and others},
  booktitle={2022 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={65},
  pages={1--3},
  year={2022},
  organization={IEEE}
}

@inproceedings{gao2024fly,
  title={On-the-Fly Data Layout Conversion for GEMM on AI Accelerators},
  author={Gao, Fang and Yue, Xingyu and Tang, Chenchen and Chen, Hongyi and Wang, Kai-Ting Amy and Abdelrahman, Tarek S},
  booktitle={2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA)},
  pages={484--493},
  year={2024},
  organization={IEEE}
}

@article{garofalo2022heterogeneous,
  title={A heterogeneous in-memory computing cluster for flexible end-to-end inference of real-world deep neural networks},
  author={Garofalo, Angelo and Ottavi, Gianmarco and Conti, Francesco and Karunaratne, Geethan and Boybat, Irem and Benini, Luca and Rossi, Davide},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={12},
  number={2},
  pages={422--435},
  year={2022},
  publisher={IEEE}
}

@inproceedings{gonugondla2020fundamental,
  title={Fundamental limits on the precision of in-memory architectures},
  author={Gonugondla, Sujan K and Sakr, Charbel and Dbouk, Hassan and Shanbhag, Naresh R},
  booktitle={Proceedings of the 39th International Conference on Computer-Aided Design},
  pages={1--9},
  year={2020}
}

@inproceedings{guo2023olive,
  title={Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization},
  author={Guo, Cong and Tang, Jiaming and Hu, Weiming and Leng, Jingwen and Zhang, Chen and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@inproceedings{han2022comprehensive,
  title={Comprehensive Design-oriented FDSOI EKV Model},
  author={Han, Hung-Chi and D'Amico, Antonio and Enz, Christian},
  booktitle={2022 29th International Conference on Mixed Design of Integrated Circuits and System (MIXDES)},
  pages={40--44},
  year={2022},
  organization={IEEE}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}

@article{hosny2023power,
  title={A power-efficient dynamic-time current mode comparator},
  author={Hosny, Ahmed and Farag, Fathi A and Wahba, Ahmed and Mohamed, Ahmed Reda},
  journal={AEU-International Journal of Electronics and Communications},
  volume={171},
  pages={154934},
  year={2023},
  publisher={Elsevier}
}

@article{houshmand2022diana,
  title={Diana: An end-to-end hybrid digital and analog neural network soc for the edge},
  author={Houshmand, Pouya and Sarda, Giuseppe M and Jain, Vikram and Ueyoshi, Kodai and Papistas, Ioannis A and Shi, Man and Zheng, Qilin and Bhattacharjee, Debjyoti and Mallik, Arindam and Debacker, Peter and others},
  journal={IEEE Journal of Solid-State Circuits},
  volume={58},
  number={1},
  pages={203--215},
  year={2022},
  publisher={IEEE}
}

@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{Hsu2019,
   abstract = {Federated Learning enables visual models to be trained in a privacy-preserving way using real-world data from mobile devices. Given their distributed nature, the statistics of the data across these devices is likely to differ significantly. In this work, we look at the effect such non-identical data distributions has on visual classification via Federated Learning. We propose a way to synthesize datasets with a continuous range of identicalness and provide performance measures for the Federated Averaging algorithm. We show that performance degrades as distributions differ more, and propose a mitigation strategy via server momentum. Experiments on CIFAR-10 demonstrate improved classification performance over a range of non-identicalness, with classification accuracy improved from 30.1% to 76.9% in the most skewed settings.},
   author = {Tzu-Ming Harry Hsu and Hang Qi and Matthew Brown},
   month = {9},
   title = {Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification},
   url = {http://arxiv.org/abs/1909.06335},
   year = {2019},
}

@article{hu2024central,
  title={The central role of tilted anisotropy for field-free spin--orbit torque switching of perpendicular magnetization},
  author={Hu, Chen-Yu and Chen, Wei-De and Liu, Yan-Ting and Huang, Chao-Chung and Pai, Chi-Feng},
  journal={NPG Asia Materials},
  volume={16},
  number={1},
  pages={1},
  year={2024},
  publisher={Springer Japan Tokyo}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{jiang2020c3sram,
  title={C3SRAM: An in-memory-computing SRAM macro based on robust capacitive coupling computing mechanism},
  author={Jiang, Zhewei and Yin, Shihui and Seo, Jae-Sun and Seok, Mingoo},
  journal={IEEE Journal of Solid-State Circuits},
  volume={55},
  number={7},
  pages={1888--1897},
  year={2020},
  publisher={IEEE}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@article{khaddam2021multi,
  title={A multi-memristive unit-cell array with diagonal interconnects for in-memory computing},
  author={Khaddam-Aljameh, Riduan and Martemucci, Michele and Kersting, Benedikt and Le Gallo, Manuel and Bruce, Robert L and BrightSky, Matthew and Sebastian, Abu},
  journal={IEEE Transactions on Circuits and Systems II: Express Briefs},
  volume={68},
  number={12},
  pages={3522--3526},
  year={2021},
  publisher={IEEE}
}

@inproceedings{koizumi2019toyadmos,
  title={ToyADMOS: A dataset of miniature-machine operating sounds for anomalous sound detection},
  author={Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Harada, Noboru and Imoto, Keisuke},
  booktitle={2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  pages={313--317},
  year={2019},
  organization={IEEE}
}

@article{koizumi2020description,
  title={Description and discussion on DCASE2020 challenge task2: Unsupervised anomalous sound detection for machine condition monitoring},
  author={Koizumi, Yuma and Kawaguchi, Yohei and Imoto, Keisuke and Nakamura, Toshiki and Nikaido, Yuki and Tanabe, Ryo and Purohit, Harsh and Suefusa, Kaori and Endo, Takashi and Yasuda, Masahiro and others},
  journal={arXiv preprint arXiv:2006.05822},
  year={2020}
}

@article{kopparapu2021tinyfedtl,
  title={TinyFedTL: Federated transfer learning on tiny devices},
  author={Kopparapu, Kavya and Lin, Eric},
  journal={arXiv preprint arXiv:2110.01107},
  year={2021}
}

@inproceedings{koryakovskiy2023one,
  title={One-shot model for mixed-precision quantization},
  author={Koryakovskiy, Ivan and Yakovleva, Alexandra and Buchnev, Valentin and Isaev, Temur and Odinokikh, Gleb},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7939--7949},
  year={2023}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{lai2018cmsis,
  title={Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus},
  author={Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
  journal={arXiv preprint arXiv:1801.06601},
  year={2018}
}

@article{lammie2024lionheart,
  title={Lionheart: A layer-based mapping framework for heterogeneous systems with analog in-memory computing tiles},
  author={Lammie, Corey and Ponzina, Flavio and Wang, Yuxuan and Klein, Joshua and Zapater, Marina and Boybat, Irem and Sebastian, Abu and Ansaloni, Giovanni and Atienza, David},
  journal={arXiv preprint arXiv:2401.09420},
  year={2024}
}

@article{le202364,
  title={A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference},
  author={Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Stanisavljevic, Milos and Vasilopoulos, Athanasios and Kersting, Benedikt and Dazzi, Martino and Karunaratne, Geethan and Br{\"a}ndli, Matthias and Singh, Abhairaj and Mueller, Silvia M and others},
  journal={Nature Electronics},
  volume={6},
  number={9},
  pages={680--693},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{lee2018unpu,
  title={UNPU: An energy-efficient deep neural network accelerator with fully variable weight bit precision},
  author={Lee, Jinmook and Kim, Changhyeon and Kang, Sanghoon and Shin, Dongjoo and Kim, Sangyeob and Yoo, Hoi-Jun},
  journal={IEEE Journal of Solid-State Circuits},
  volume={54},
  number={1},
  pages={173--185},
  year={2018},
  publisher={IEEE}
}

@inproceedings{lee2019monocular,
  title={Monocular depth estimation using relative depth maps},
  author={Lee, Jae-Han and Kim, Chang-Su},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2019}
}

@inproceedings{leroy2019federated,
  title={Federated learning for keyword spotting},
  author={Leroy, David and Coucke, Alice and Lavril, Thibaut and Gisselbrecht, Thibault and Dureau, Joseph},
  booktitle={ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={6341--6345},
  year={2019},
  organization={IEEE}
}

@article{li2020deep,
  title={The deep learning compiler: A comprehensive survey},
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={3},
  pages={708--727},
  year={2020},
  publisher={IEEE}
}

@article{lin2020mcunet,
  title={Mcunet: Tiny deep learning on iot devices},
  author={Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11711--11722},
  year={2020}
}


@phdthesis{lin2021efficient,
  title={Efficient Algorithms and Systems for Tiny Deep Learning},
  author={Lin, Ji},
  year={2021},
  school={Massachusetts Institute of Technology}
}

@article{lin2021memory,
  title={Memory-efficient patch-based inference for tiny deep learning},
  author={Lin, Ji and Chen, Wei-Ming and Cai, Han and Gan, Chuang and Han, Song},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2346--2358},
  year={2021}
}

@article{lin2022device,
  title={On-device training under 256kb memory},
  author={Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22941--22954},
  year={2022}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{McmahanFedAvg,
   abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model ar-chitectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100× as compared to synchronized stochastic gradient descent.},
   author = {H Brendan McMahan Eider Moore Daniel Ramage Seth Hampson Blaise AgüeraAg and Agüera Arcas},
   title = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
   year = {2017},
}

@article{mei2021zigzag,
  title={ZigZag: Enlarging joint architecture-mapping design space exploration for DNN accelerators},
  author={Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
  journal={IEEE Transactions on Computers},
  volume={70},
  number={8},
  pages={1160--1174},
  year={2021},
  publisher={IEEE}
}

@inproceedings{mei2022uniform,
  title={A uniform latency model for DNN accelerators with diverse architectures and dataflows},
  author={Mei, Linyan and Liu, Huichu and Wu, Tony and Sumbul, H Ekin and Verhelst, Marian and Beigne, Edith},
  booktitle={2022 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={220--225},
  year={2022},
  organization={IEEE}
}

@inproceedings{mei2023defines,
  title={Defines: Enabling fast exploration of the depth-first scheduling space for dnn accelerators through analytical modeling},
  author={Mei, Linyan and Goetschalckx, Koen and Symons, Arne and Verhelst, Marian},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={570--583},
  year={2023},
  organization={IEEE}
}

@article{mei2023design,
  title={Design Space Exploration of Deep Learning Accelerators},
  author={Mei, Linyan and Verhelst, Marian},
  year={2023}
}

@inproceedings{nest2024casting,
  title={Casting hybrid digital-analog training into hierarchical energy-based learning},
  author={Nest, Timothy and Ernoult, Maxence},
  booktitle={NeurIPS 2024 Workshop Machine Learning with new Compute Paradigms}
}

@misc{onnxruntime,
  title={ONNX Runtime},
  author={ONNX Runtime developers},
  year={2021},
  howpublished={\url{https://onnxruntime.ai/}},
  note={Version: x.y.z}
}

@article{palossi201964,
  title={A 64-mw dnn-based visual navigation engine for autonomous nano-drones},
  author={Palossi, Daniele and Loquercio, Antonio and Conti, Francesco and Flamand, Eric and Scaramuzza, Davide and Benini, Luca},
  journal={IEEE Internet of Things Journal},
  volume={6},
  number={5},
  pages={8357--8371},
  year={2019},
  publisher={IEEE}
}

@inproceedings{peng2019dnn+,
  title={DNN+ NeuroSim: An end-to-end benchmarking framework for compute-in-memory accelerators with versatile device technologies},
  author={Peng, Xiaochen and Huang, Shanshi and Luo, Yandong and Sun, Xiaoyu and Yu, Shimeng},
  booktitle={2019 IEEE international electron devices meeting (IEDM)},
  pages={32--5},
  year={2019},
  organization={IEEE}
}

@article{peng2020dnn+,
  title={DNN+ NeuroSim V2. 0: An end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training},
  author={Peng, Xiaochen and Huang, Shanshi and Jiang, Hongwu and Lu, Anni and Yu, Shimeng},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={40},
  number={11},
  pages={2306--2319},
  year={2020},
  publisher={IEEE}
}

@article{perumal2023tunable,
  title={Tunable conversion of topological spin texture from domain wall pair for magnetic memory application in specially designed magnetic nanotracks},
  author={Perumal, Hari Prasanth and Sankaran Kunnath, Syamlal and Priyanka, Babu and Sinha, Jaivardhan},
  journal={ACS Applied Electronic Materials},
  volume={5},
  number={7},
  pages={3641--3649},
  year={2023},
  publisher={ACS Publications}
}

@article{pourmand2024laplace,
  title={Laplace-HDC: Understanding the geometry of binary hyperdimensional computing},
  author={Pourmand, Saeid and Whiting, Wyatt D and Aghasi, Alireza and Marshall, Nicholas F},
  journal={arXiv preprint arXiv:2404.10759},
  year={2024}
}

@article{prabhu2022chimera,
  title={CHIMERA: A 0.92-TOPS, 2.2-TOPS/W edge AI accelerator with 2-MByte on-chip foundry resistive RAM for efficient training and inference},
  author={Prabhu, Kartik and Gural, Albert and Khan, Zainab F and Radway, Robert M and Giordano, Massimo and Koul, Kalhan and Doshi, Rohan and Kustin, John W and Liu, Timothy and Lopes, Gregorio B and others},
  journal={IEEE Journal of Solid-State Circuits},
  volume={57},
  number={4},
  pages={1013--1026},
  year={2022},
  publisher={IEEE}
}
@article{radway2021illusion,
  title={Illusion of large on-chip memory by networked computing chips for neural network inference},
  author={Radway, Robert M and Bartolo, Andrew and Jolly, Paul C and Khan, Zainab F and Le, Binh Q and Tandon, Pulkit and Wu, Tony F and Xin, Yunfeng and Vianello, Elisa and Vivet, Pascal and others},
  journal={Nature Electronics},
  volume={4},
  number={1},
  pages={71--80},
  year={2021},
  publisher={Nature Publishing Group UK London}
}@article{raghavan2009empire,
  title={EMPIRE: Empirical power/area/timing models for register files},
  author={Raghavan, Praveen and Lambrechts, Andy and Jayapala, Murali and Catthoor, Francky and Verkest, Diederik},
  journal={Microprocessors and Microsystems},
  volume={33},
  number={4},
  pages={295--300},
  year={2009},
  publisher={Elsevier}
}@article{rakka2024review,
  title={A review of state-of-the-art mixed-precision neural network frameworks},
  author={Rakka, Mariam and Fouda, Mohammed E and Khargonekar, Pramod and Kurdahi, Fadi},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}@article{Ren2023,
   abstract = {Tiny machine learning (TinyML) is a rapidly growing field aiming to democratize machine learning (ML) for resource-constrained microcontrollers (MCUs). Given the pervasiveness of these tiny devices, it is inherent to ask whether TinyML applications can benefit from aggregating their knowledge. Federated learning (FL) enables decentralized agents to jointly learn a global model without sharing sensitive local data. However, a common global model may not work for all devices due to the complexity of the actual deployment environment and the heterogeneity of the data available on each device. In addition, the deployment of TinyML hardware has significant computational and communication constraints, which traditional ML fails to address. Considering these challenges, we propose TinyReptile, a simple but efficient algorithm inspired by meta-learning and online learning, to collaboratively learn a solid initialization for a neural network (NN) across tiny devices that can be quickly adapted to a new device with respect to its data. We demonstrate TinyReptile on Raspberry Pi 4 and Cortex-M4 MCU with only 256-KB RAM. The evaluations on various TinyML use cases confirm a resource reduction and training time saving by at least two factors compared with baseline algorithms with comparable performance.},
   author = {Haoyu Ren and Darko Anicic and Thomas A. Runkler},
   month = {4},
   title = {TinyReptile: TinyML with Federated Meta-Learning},
   url = {http://arxiv.org/abs/2304.05201},
   year = {2023},
}@article{ren2023tinymetafed,
  title={TinyMetaFed: Efficient Federated Meta-Learning for TinyML},
  author={Ren, Haoyu and Li, Xue and Anicic, Darko and Runkler, Thomas A},
  journal={arXiv preprint arXiv:2307.06822},
  year={2023}
}@article{ren2023tinyreptile,
  title={TinyReptile: TinyML with Federated Meta-Learning},
  author={Ren, Haoyu and Anicic, Darko and Runkler, Thomas A},
  journal={arXiv preprint arXiv:2304.05201},
  year={2023}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@mastersthesis{sandtke2021solving,
  title={Solving Cutting and Packing Problems With Neural Networks},
  author={Sandtke, WD},
  year={2021}
}@article{sebastian2020memory,
  title={Memory devices and applications for in-memory computing},
  author={Sebastian, Abu and Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Eleftheriou, Evangelos},
  journal={Nature nanotechnology},
  volume={15},
  number={7},
  pages={529--544},
  year={2020},
  publisher={Nature Publishing Group UK London}
}@article{shanbhag2022benchmarking,
  title={Benchmarking in-memory computing architectures},
  author={Shanbhag, Naresh R and Roy, Saion K},
  journal={IEEE Open Journal of the Solid-State Circuits Society},
  volume={2},
  pages={288--300},
  year={2022},
  publisher={IEEE}
}@article{sharma2021electrically,
  title={Electrically connected spin-torque oscillators array for 2.4 GHz WiFi band transmission and energy harvesting},
  author={Sharma, Raghav and Mishra, Rahul and Ngo, Tung and Guo, Yong-Xin and Fukami, Shunsuke and Sato, Hideo and Ohno, Hideo and Yang, Hyunsoo},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={2924},
  year={2021},
  publisher={Nature Publishing Group UK London}
}@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}@article{song2023recent,
  title={Recent advances and future prospects for memristive materials, devices, and systems},
  author={Song, Min-Kyu and Kang, Ji-Hoon and Zhang, Xinyuan and Ji, Wonjae and Ascoli, Alon and Messaris, Ioannis and Demirkol, Ahmet Samil and Dong, Bowei and Aggarwal, Samarth and Wan, Weier and others},
  journal={ACS nano},
  volume={17},
  number={13},
  pages={11994--12039},
  year={2023},
  publisher={American Chemical Society}
}@article{tsukada2020neural,
  title={A neural network-based on-device learning anomaly detector for edge devices},
  author={Tsukada, Mineto and Kondo, Masaaki and Matsutani, Hiroki},
  journal={IEEE Transactions on Computers},
  volume={69},
  number={7},
  pages={1027--1044},
  year={2020},
  publisher={IEEE}
}@inproceedings{wang2020always,
  title={Always-on, sub-300-nw, event-driven spiking neural network based on spike-driven clock-generation and clock-and power-gating for an ultra-low-power intelligent device},
  author={Wang, Dewei and Chundi, Pavan Kumar and Kim, Sung Justin and Yang, Minhao and Cerqueira, Joao Pedro and Kang, Joonsung and Jung, Seungchul and Kim, Sangjoon and Seok, Mingoo},
  booktitle={2020 IEEE Asian Solid-State Circuits Conference (A-SSCC)},
  pages={1--4},
  year={2020},
  organization={IEEE}
}@inproceedings{wang2021brief,
  title={Brief industry paper: Hdad: Hyperdimensional computing-based anomaly detection for automotive sensor attacks},
  author={Wang, Ruixuan and Kong, Fanxin and Sudler, Hasshi and Jiao, Xun},
  booktitle={2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS)},
  pages={461--464},
  year={2021},
  organization={IEEE}
}@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}@article{wanneurram,
  title={Neurram: Rram compute-in-memory chip for efficient versatile and accurate ai inference},
  author={Wan, Weier and Kubendran, Rajkumar and Schaefer, Clemens and Eryilmaz, S Burc and Zhang, Wenqiang and Wu, Dabin and Deiss, Stephen and Raina, Priyanka and Qian, He and Gao, Bin and others}
}@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}@article{xu2021federated,
  title={Federated learning for healthcare informatics},
  author={Xu, Jie and Glicksberg, Benjamin S and Su, Chang and Walker, Peter and Bian, Jiang and Wang, Fei},
  journal={Journal of Healthcare Informatics Research},
  volume={5},
  pages={1--19},
  year={2021},
  publisher={Springer}
}@inproceedings{yan2019rram,
  title={RRAM-based spiking nonvolatile computing-in-memory processing engine with precision-configurable in situ nonlinear activation},
  author={Yan, Bonan and Yang, Qing and Chen, Wei-Hao and Chang, Kung-Tang and Su, Jian-Wei and Hsu, Chien-Hua and Li, Sih-Han and Lee, Heng-Yuan and Sheu, Shyh-Shyuan and Ho, Mon-Shu and others},
  booktitle={2019 Symposium on VLSI Technology},
  pages={T86--T87},
  year={2019},
  organization={IEEE}
}@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}@inproceedings{yang2020interstellar,
  title={Interstellar: Using halide's scheduling language to analyze dnn accelerators},
  author={Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff and Pu, Jing and Nayak, Ankita and Bell, Steven and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and others},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={369--383},
  year={2020}
}@article{yin2020xnor,
  title={XNOR-SRAM: In-memory computing SRAM macro for binary/ternary deep neural networks},
  author={Yin, Shihui and Jiang, Zhewei and Seo, Jae-Sun and Seok, Mingoo},
  journal={IEEE Journal of Solid-State Circuits},
  volume={55},
  number={6},
  pages={1733--1743},
  year={2020},
  publisher={IEEE}
}@inproceedings{zhang2015optimizing,
  title={Optimizing FPGA-based accelerator design for deep convolutional neural networks},
  author={Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
  booktitle={Proceedings of the 2015 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={161--170},
  year={2015}
}@article{zhang2017hello,
  title={Hello edge: Keyword spotting on microcontrollers},
  author={Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
  journal={arXiv preprint arXiv:1711.07128},
  year={2017}
}@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}
@inproceedings{zhao2022reinforcement,
  title={A reinforcement learning algorithm for the 2D-rectangular strip packing problem},
  author={Zhao, Xusheng and Rao, Yunqing and Fang, Jie},
  booktitle={Journal of Physics: Conference Series},
  volume={2181},
  number={1},
  pages={012002},
  year={2022},
  organization={IOP Publishing}
}
@article{zhou2019edge,
  title={Edge intelligence: Paving the last mile of artificial intelligence with edge computing},
  author={Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
  journal={Proceedings of the IEEE},
  volume={107},
  number={8},
  pages={1738--1762},
  year={2019},
  publisher={IEEE}
}
@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21285--21296},
  year={2020}
}
@article{zhou2022ml,
  title={Ml-hw co-design of noise-robust tinyml models and always-on analog compute-in-memory edge accelerator},
  author={Zhou, Chuteng and Redondo, Fernando Garcia and B{\"u}chel, Julian and Boybat, Irem and Comas, Xavier Timoneda and Nandakumar, SR and Das, Shidhartha and Sebastian, Abu and Le Gallo, Manuel and Whatmough, Paul N},
  journal={IEEE Micro},
  volume={42},
  number={6},
  pages={76--87},
  year={2022},
  publisher={IEEE}
}@inproceedings{zyuban1998energy,
  title={The energy complexity of register files},
  author={Zyuban, Victor and Kogge, Peter},
  booktitle={Proceedings of the 1998 international symposium on Low power electronics and design},
  pages={305--310},
  year={1998}
}
@article{symons2024stream,
  title={Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators},
  author={Symons, Arne and Mei, Linyan and Colleman, Steven and Houshmand, Pouya and Karl, Sebastian and Verhelst, Marian},
  journal={IEEE Transactions on Computers},
  year={2024},
  publisher={IEEE}
}@inproceedings{andrulis2024cimloop,
  title={CiMLoop: A flexible, accurate, and fast compute-in-memory modeling tool},
  author={Andrulis, Tanner and Emer, Joel S and Sze, Vivienne},
  booktitle={2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={10--23},
  year={2024},
  organization={IEEE}
}@inproceedings{parashar2019timeloop,
  title={Timeloop: A systematic approach to dnn accelerator evaluation},
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W and Emer, Joel},
  booktitle={2019 IEEE international symposium on performance analysis of systems and software (ISPASS)},
  pages={304--315},
  year={2019},
  organization={IEEE}
}
@article{meng2024compute,
  title={Compute-in-memory technologies for deep learning acceleration},
  author={Meng, Fan-husan and Lu, Wei D},
  journal={IEEE Nanotechnology Magazine},
  volume={18},
  number={1},
  pages={44--52},
  year={2024},
  publisher={IEEE}
}@article{jylanki2010thousand,
  title={A thousand ways to pack the bin-a practical approach to two-dimensional rectangle bin packing},
  author={Jyl{\"a}nki, Jukka},
  journal={retrived from http://clb. demon. fi/files/RectangleBinPack. pdf},
  year={2010}
}@article{murmann2020mixed,
  title={Mixed-signal computing for deep neural network inference},
  author={Murmann, Boris},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={29},
  number={1},
  pages={3--13},
  year={2020},
  publisher={IEEE}
}